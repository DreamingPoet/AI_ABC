# model_name: str = "deepseek-r1:7b", base_url: str = "http://192.168.0.245:11434"

import subprocess
import json
# from openai import OpenAI # Removed OpenAI import
import ollama # Added ollama import

# Default configuration (can be overridden)
DEFAULT_MODEL_NAME = "deepseek-r1:7b" # Or your specific local model identifier
# Adjusted default base URL for ollama library (no /v1)
DEFAULT_BASE_URL = "http://192.168.0.245:11434" # Standard Ollama API endpoint path

def execute_script_with_llm(
    script_path: str,
    params: dict = None,
    model_name: str = DEFAULT_MODEL_NAME,
    base_url: str = DEFAULT_BASE_URL,
    # api_key: str = "ollama", # API key not typically used directly with ollama library
) -> tuple[str, str, str]:
    """
    Uses a local Ollama LLM (via ollama library) to generate a command for executing a Python script with parameters,
    then executes the command locally and returns the output.

    Args:
        script_path: Path to the Python script to execute.
        params: A dictionary of parameters to pass to the script.
                Keys will be argument names (e.g., "--input"), values will be argument values.
                For flags (arguments without values), set the value to None or True.
        model_name: The identifier of the local Ollama LLM model.
        base_url: The base URL (host) of the local Ollama API endpoint (e.g., "http://localhost:11434").
        # api_key: API key is generally not needed for local Ollama via this library.

    Returns:
        A tuple containing:
        - The command generated by the LLM (str).
        - The standard output of the executed script (str).
        - The standard error of the executed script (str).
    """
    if params is None:
        params = {}

    # Initialize Ollama client
    # Ensure base_url doesn't end with /v1 or similar paths
    if base_url.endswith('/v1'):
        host_url = base_url[:-3]
    elif base_url.endswith('/'):
         host_url = base_url[:-1]
    else:
        host_url = base_url

    try:
        client = ollama.Client(host=host_url)
    except Exception as e:
         print(f"Error initializing Ollama client with host '{host_url}': {e}")
         return "", "", f"Error initializing Ollama client: {e}"


    # Construct the parameter string for the prompt
    param_parts = []
    for key, value in params.items():
        if value is None or value is True: # Handle flags
             param_parts.append(f"{key}")
        else:
             # Ensure string values are quoted if they contain spaces
             value_str = str(value)
             if ' ' in value_str:
                 param_parts.append(f'{key} "{value_str}"')
             else:
                 param_parts.append(f"{key} {value_str}")
    params_str = " ".join(param_parts)

    prompt = f"""
    Generate the exact command-line instruction to execute the Python script located at '{script_path}'.
    The script should be run with the following arguments: {params_str}
    Only output the complete command line string, starting with 'python' or 'python3', and nothing else.
    For example: python {script_path} {params_str}
    """

    try:
        # Use ollama client's chat method
        response = client.chat(
            model=model_name,
            messages=[
                {'role': 'system', 'content': 'You are a helpful assistant that generates command-line instructions.'},
                {'role': 'user', 'content': prompt}
            ],
            options={ # Options equivalent to temperature, max_tokens etc.
                'temperature': 0.1,
                # 'num_predict': 150 # Equivalent to max_tokens, adjust if needed
            }
        )
        # Extract content from the ollama response structure
        generated_command = response['message']['content'].strip()

        # Basic validation/cleanup - remove potential markdown backticks
        if generated_command.startswith("`") and generated_command.endswith("`"):
            generated_command = generated_command.strip("`")
        if generated_command.startswith("```") and generated_command.endswith("```"):
             generated_command = generated_command.splitlines()[1 if generated_command.startswith("```python") else 0].strip()


        # Ensure the command starts with python or python3
        if not (generated_command.startswith("python ") or generated_command.startswith("python3 ")):
             # Fallback or raise error - maybe prepend python?
             print(f"Warning: LLM generated command doesn't start with python/python3: '{generated_command}'. Prepending 'python'.")
             if not generated_command.startswith(script_path): # Avoid "python python script.py"
                 generated_command = f"python {generated_command}"


    except Exception as e:
        print(f"Error interacting with Ollama LLM: {e}")
        return "", "", f"Error interacting with Ollama LLM: {e}"

    # --- Execute the generated command ---
    try:
        # Using shell=True can be a security risk if the command is not trusted.
        # Here we trust the command generated based on our specific prompt structure,
        # but be cautious in other contexts.
        # We split the command for better cross-platform compatibility if possible,
        # but complex commands with quotes might require shell=True.
        # Let's try executing directly first for security.
        # result = subprocess.run(generated_command.split(), capture_output=True, text=True, check=False)

        # Using shell=True as LLM might generate complex commands with pipes or quotes
        # Be aware of the security implications.
        print(f"Executing command: {generated_command}")
        result = subprocess.run(generated_command, capture_output=True, text=True, check=False, shell=True)

        stdout = result.stdout
        stderr = result.stderr

        return generated_command, stdout, stderr

    except FileNotFoundError:
         stderr_msg = f"Error: The command '{generated_command.split()[0]}' was not found. Is Python installed and in your PATH?"
         print(stderr_msg)
         return generated_command, "", stderr_msg
    except Exception as e:
        stderr_msg = f"Error executing command '{generated_command}': {e}"
        print(stderr_msg)
        return generated_command, "", stderr_msg

# --- Example Usage ---
if __name__ == "__main__":
    # Create a dummy target script for testing
    target_script_content = """
import argparse
import json

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Test script.')
    parser.add_argument('--input', type=str, required=True, help='Input file path')
    parser.add_argument('--output', type=str, default='output.txt', help='Output file path')
    parser.add_argument('--threshold', type=float, default=0.5, help='A threshold value')
    parser.add_argument('--verbose', action='store_true', help='Enable verbose output')
    parser.add_argument('--names', nargs='+', default=['default'], help='List of names')

    args = parser.parse_args()

    print(f"Executing test script...")
    print(f"Input: {args.input}")
    print(f"Output: {args.output}")
    print(f"Threshold: {args.threshold}")
    print(f"Verbose: {args.verbose}")
    print(f"Names: {args.names}")

    # Example of outputting structured data (e.g., JSON)
    result_data = {
        'status': 'success',
        'processed_input': args.input,
        'config': {
            'output_file': args.output,
            'threshold': args.threshold,
            'verbose_mode': args.verbose,
            'names_list': args.names
        }
    }
    print("---JSON START---")
    print(json.dumps(result_data, indent=2))
    print("---JSON END---")

    # You could also write to stderr
    # import sys
    # print("This is a test error message.", file=sys.stderr)
"""
    target_script_path = "dummy_target_script.py"
    with open(target_script_path, "w") as f:
        f.write(target_script_content)

    print(f"Created dummy script: {target_script_path}")

    # --- Define parameters for the dummy script ---
    script_params = {
        "--input": "data/my_input.csv",
        "--output": "results/processed_data.json",
        "--threshold": 0.85,
        "--verbose": True, # Flag argument
        "--names": "Alice Bob Charlie" # Pass list as space-separated string
        # Alternatively for names: "--names": ['Alice', 'Bob', 'Charlie'] -> adjust prompt/parsing if needed
    }

    # --- Call the function ---
    print("\n--- Calling LLM to execute script ---")
    # Make sure your local LLM server (Ollama) is running
    # and accessible at the specified base_url.
    # You might need to adjust DEFAULT_BASE_URL and DEFAULT_MODEL_NAME above.
    try:
        command, stdout, stderr = execute_script_with_llm(target_script_path, script_params)

        print("\n--- Results ---")
        print(f"LLM Generated Command:\n{command}")
        print("\nScript Standard Output:")
        print(stdout)
        if stderr:
            print("\nScript Standard Error:")
            print(stderr)

        # Example: Try to parse JSON from stdout if expected
        try:
            json_output = stdout.split("---JSON START---")[1].split("---JSON END---")[0]
            parsed_result = json.loads(json_output.strip())
            print("\nParsed JSON Result:")
            print(parsed_result)
        except (IndexError, json.JSONDecodeError) as e:
            print(f"\nCould not parse JSON from output: {e}")


    except Exception as e:
        print(f"\nAn error occurred during the process: {e}")

    # Clean up the dummy script (optional)
    # import os
    # os.remove(target_script_path)
    # print(f"\nCleaned up dummy script: {target_script_path}")

